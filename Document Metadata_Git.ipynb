{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d5b7d-fa39-4485-a2a7-4482efe9dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nest_asyncio PyMuPDF pandas llama-index llama-index-extractors-entity\n",
    "\n",
    "#Document Metadata using SimpleDirectoryReader\n",
    "#Ensure to install the required packages\n",
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    SummaryExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Path to the local directory containing the PDF files\n",
    "local_directory = \"/Users/pradhikshasuresh/Documents/Python/Chatbot\"\n",
    "\n",
    "# Use SimpleDirectoryReader to load the documents from the local directory\n",
    "reader = SimpleDirectoryReader(input_dir=local_directory)  # Adjust chunk_size as needed\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Limit to the first 2 documents\n",
    "documents = documents[:3]\n",
    "\n",
    "# Setup OpenAPI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"My_OpenAI_key\"\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "# Define Extractors\n",
    "entity_extractor = EntityExtractor(\n",
    "    prediction_threshold=0.5,\n",
    "    label_entities=False,  \n",
    "    device=\"cpu\",  \n",
    ")\n",
    "qa_extractor = QuestionsAnsweredExtractor(questions=3, llm=llm)\n",
    "summary_extractor = SummaryExtractor(summaries=[\"self\"], llm=llm)\n",
    "title_extractor = TitleExtractor(nodes=5, llm=llm)\n",
    "keyword_extractor = KeywordExtractor(keywords=10, llm=llm)\n",
    "node_parser = SentenceSplitter()\n",
    "\n",
    "transformations = [node_parser, title_extractor, entity_extractor, summary_extractor, qa_extractor, keyword_extractor]\n",
    "\n",
    "# Create the ingestion pipeline\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "# Run the pipeline to extract metadata\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "# Prepare data for CSV\n",
    "metadata_list = []\n",
    "for node in nodes:\n",
    "    metadata = node.metadata\n",
    "    metadata_list.append(metadata)\n",
    "\n",
    "# Convert metadata to a DataFrame\n",
    "df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "output_csv_path = \"docmetadata_output.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Print the location where the CSV is saved\n",
    "print(f\"Metadata has been saved to {output_csv_path}\")\n",
    "print(f\"Full path: {os.path.abspath(output_csv_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a39dd5-a6e0-46ed-9016-b30e97a78bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document metadata using pdf loader\n",
    "import os\n",
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    SummaryExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import fitz  # PyMuPDF for PDF handling\n",
    "import time  # Import time module for sleep function\n",
    "\n",
    "# Path to the local directory containing the PDF files\n",
    "local_directory = \"/Users/pradhikshasuresh/Documents/Python/Chatbot\"\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [file for file in os.listdir(local_directory) if file.endswith(\".pdf\")]\n",
    "\n",
    "# Limit to the first two PDF files for testing\n",
    "pdf_files = pdf_files[:2]\n",
    "\n",
    "# Setup OpenAI key if needed\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"My_OpenAI_key\"\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "# Define Extractors\n",
    "entity_extractor = EntityExtractor(\n",
    "    prediction_threshold=0.5,\n",
    "    label_entities=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "qa_extractor = QuestionsAnsweredExtractor(questions=3, llm=llm)\n",
    "summary_extractor = SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm)\n",
    "title_extractor = TitleExtractor(nodes=5, llm=llm)\n",
    "keyword_extractor = KeywordExtractor(keywords=10, llm=llm)\n",
    "node_parser = SentenceSplitter()\n",
    "\n",
    "transformations = [node_parser, title_extractor, entity_extractor, summary_extractor, qa_extractor, keyword_extractor]\n",
    "\n",
    "# Create the ingestion pipeline\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "# Prepare to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in pdf_files:\n",
    "    # Construct full path to the PDF file\n",
    "    pdf_path = os.path.join(local_directory, pdf_file)\n",
    "    \n",
    "    # Load the PDF content using PyMuPDF (fitz)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pdf_text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        pdf_text += page.get_text()\n",
    "    \n",
    "    # Create Document object for the PDF\n",
    "    document = Document(text=pdf_text)\n",
    "    \n",
    "    # Run the ingestion pipeline to extract metadata\n",
    "    nodes = pipeline.run(documents=[document])\n",
    "    \n",
    "    # Store metadata\n",
    "    metadata_list = []\n",
    "    for node in nodes:\n",
    "        metadata = node.metadata\n",
    "        metadata_list.append(metadata)\n",
    "\n",
    "# Convert metadata to a DataFrame\n",
    "df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "output_csv_path = \"docmetadata1_output.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Print the location where the CSV is saved\n",
    "print(f\"Metadata has been saved to {output_csv_path}\")\n",
    "print(f\"Full path: {os.path.abspath(output_csv_path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
